name: Weekly Dataset Consolidation

on:
  schedule:
    # Run Monday 2 AM UTC â€” after daily (4 AM Sun) and weekly entscheidsuche (5 AM Sun)
    - cron: '0 2 * * 1'
  workflow_dispatch:

env:
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
  LOG_LEVEL: INFO

# Prevent concurrent runs of data update workflows
concurrency:
  group: data-update
  cancel-in-progress: false

jobs:
  consolidate:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: pipeline/requirements.txt

      - name: Install pipeline deps
        run: |
          python -m pip install --upgrade pip
          pip install -r pipeline/requirements.txt
          pip install -e pipeline

      - name: Consolidate data shards
        run: |
          python -m caselaw_pipeline.cli consolidate-data \
            --build-dir _build

      - name: Update SQLite snapshot
        run: |
          python -m caselaw_pipeline.cli consolidate-weekly \
            --build-dir _build \
            --zstd-level 10
        # Only run if a snapshot already exists in the manifest.
        # To bootstrap the first snapshot, run build-snapshot + publish-snapshot locally.
        continue-on-error: true

      - name: Write job summary
        run: |
          echo "## Weekly Dataset Consolidation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Consolidated delta parquet files in data/" >> $GITHUB_STEP_SUMMARY
          echo "- Updated SQLite snapshot (if available)" >> $GITHUB_STEP_SUMMARY
          echo "- Updated id-index.parquet" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- [HuggingFace Dataset](https://huggingface.co/datasets/voilaj/swiss-caselaw)" >> $GITHUB_STEP_SUMMARY
