name: Weekly Entscheidsuche Gap Fill

on:
  schedule:
    # Run weekly on Sunday at 5 AM UTC (after verify-gaps at 6 AM, before daily at 4 AM Monday)
    - cron: '0 5 * * 0'
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run (count only, no imports)'
        required: false
        default: false
        type: boolean
      canton:
        description: 'Specific canton to import (e.g., ZH, GE). Leave empty for all.'
        required: false
        default: ''
        type: string

env:
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
  DATABASE_URL: postgresql+psycopg://postgres:postgres@localhost:5432/swisslaw
  PYTHONPATH: ${{ github.workspace }}/backend

# Prevent concurrent runs of data update workflows
concurrency:
  group: data-update
  cancel-in-progress: false

jobs:
  entscheidsuche-import:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: swisslaw
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies
        working-directory: ./backend
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Initialize database schema
        working-directory: ./backend
        run: |
          alembic upgrade head

      # NOTE: No HF import â€” CI imports from entscheidsuche.ch directly.
      # Results are published as deltas to HuggingFace.

      - name: Run entscheidsuche import
        working-directory: ./backend
        env:
          DRY_RUN: ${{ github.event.inputs.dry_run || 'false' }}
          CANTON: ${{ github.event.inputs.canton || '' }}
        run: |
          echo "Running entscheidsuche.ch import..."

          ARGS=""
          if [ "$DRY_RUN" = "true" ]; then
            ARGS="$ARGS --dry-run"
          fi
          if [ -n "$CANTON" ]; then
            ARGS="$ARGS --canton $CANTON"
          fi

          python scripts/import_entscheidsuche_robust.py $ARGS

      - name: Get decision count
        id: stats
        working-directory: ./backend
        run: |
          python -c "
          import sys, os
          sys.path.insert(0, '.')
          from sqlmodel import select, func
          from app.db.session import get_session
          from app.models.decision import Decision
          with get_session() as session:
              total = session.exec(select(func.count(Decision.id))).one()
          print(f'Decisions imported: {total}')
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={total}\n')
          "

      - name: Export decisions
        if: github.event.inputs.dry_run != 'true'
        working-directory: ./backend
        run: python scripts/export_decisions.py ../data/exports/decisions.json.gz

      - name: Install pipeline deps
        if: github.event.inputs.dry_run != 'true'
        run: |
          pip install -r pipeline/requirements.txt
          pip install -e pipeline

      - name: Build delta
        if: github.event.inputs.dry_run != 'true'
        run: |
          DATE="$(date -u +%F)"
          python -m caselaw_pipeline.cli build-delta --export data/exports/decisions.json.gz --out _build --date "$DATE" --parquet

      - name: Publish delta
        if: github.event.inputs.dry_run != 'true'
        run: |
          DATE="$(date -u +%F)"
          python -m caselaw_pipeline.cli publish-delta --build-dir _build --date "$DATE" --parquet

      - name: Append delta to dataset
        if: github.event.inputs.dry_run != 'true'
        run: |
          DATE="$(date -u +%F)"
          python -m caselaw_pipeline.cli append-to-data --build-dir _build --date "$DATE"

      - name: Write job summary
        run: |
          echo "## Weekly Entscheidsuche Import Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|------:|" >> $GITHUB_STEP_SUMMARY
          echo "| Decisions Imported | ${{ steps.stats.outputs.total }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ "${{ github.event.inputs.dry_run }}" = "true" ]; then
            echo "This was a dry run - no data was published." >> $GITHUB_STEP_SUMMARY
          else
            echo "Delta published to HuggingFace." >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- [HuggingFace Dataset](https://huggingface.co/datasets/voilaj/swiss-caselaw)" >> $GITHUB_STEP_SUMMARY
