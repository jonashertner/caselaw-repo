name: Daily Update

on:
  schedule:
    # Run daily at 4 AM UTC (give courts time to publish)
    - cron: '0 4 * * *'
  workflow_dispatch:
    inputs:
      days:
        description: 'Number of days to scrape (default: 7)'
        required: false
        default: '7'
        type: string

env:
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
  DATABASE_URL: postgresql+psycopg://postgres:postgres@localhost:5432/swisslaw
  PYTHONPATH: ${{ github.workspace }}/backend
  LOG_LEVEL: INFO

# Prevent concurrent runs of data update workflows
concurrency:
  group: data-update
  cancel-in-progress: false

jobs:
  update-dataset:
    runs-on: ubuntu-latest
    timeout-minutes: 180

    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: swisslaw
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      # ── Scraping & DB steps ──────────────────────────────────────────
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt

      - name: Install backend dependencies
        working-directory: ./backend
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: |
          pip install playwright
          playwright install chromium --with-deps || echo "Playwright install failed, continuing without browser automation"

      - name: Initialize database schema
        working-directory: ./backend
        run: alembic upgrade head

      # NOTE: No HF import — CI scrapes only new decisions (last N days).
      # Full dataset lives on HuggingFace; deltas are published incrementally.

      - name: Run incremental update
        working-directory: ./backend
        env:
          DAYS: ${{ github.event.inputs.days || '7' }}
        run: |
          echo "Running incremental update for last $DAYS days..."
          python scripts/daily_update.py --days $DAYS

      - name: Generate statistics
        id: stats
        working-directory: ./backend
        run: |
          python -c "
          import sys
          import os
          sys.path.insert(0, '.')
          from sqlmodel import select, func
          from app.db.session import get_session
          from app.models.decision import Decision

          with get_session() as session:
              total = session.exec(select(func.count(Decision.id))).one()
              federal = session.exec(
                  select(func.count(Decision.id)).where(Decision.level == 'federal')
              ).one()
              cantonal = session.exec(
                  select(func.count(Decision.id)).where(Decision.level == 'cantonal')
              ).one()

              canton_query = (
                  select(Decision.canton, func.count(Decision.id))
                  .where(Decision.canton.isnot(None))
                  .group_by(Decision.canton)
                  .order_by(func.count(Decision.id).desc())
              )
              cantons = session.exec(canton_query).all()

              lang_query = (
                  select(Decision.language, func.count(Decision.id))
                  .where(Decision.language.isnot(None))
                  .group_by(Decision.language)
              )
              langs = dict(session.exec(lang_query).all())

          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={total}\n')
              f.write(f'federal={federal}\n')
              f.write(f'cantonal={cantonal}\n')
              f.write(f'cantons={len(cantons)}\n')
              f.write(f'german={langs.get(\"de\", 0)}\n')
              f.write(f'french={langs.get(\"fr\", 0)}\n')
              f.write(f'italian={langs.get(\"it\", 0)}\n')

          print(f'Total: {total}')
          print(f'Federal: {federal}')
          print(f'Cantonal: {cantonal}')
          print(f'Cantons covered: {len(cantons)}')
          "

      - name: Export decisions
        working-directory: ./backend
        run: python scripts/export_decisions.py ../data/exports/decisions.json.gz

      # ── Pipeline steps: build and publish incremental deltas ─────────
      # NOTE: Full dataset pushes (parquet, SQLite, JSON) are done locally,
      # not from CI, because CI starts from an empty DB each run.
      - name: Install pipeline deps
        run: |
          pip install -r pipeline/requirements.txt
          pip install -e pipeline

      - name: Build delta
        run: |
          DATE="$(date -u +%F)"
          python -m caselaw_pipeline.cli build-delta --export data/exports/decisions.json.gz --out _build --date "$DATE" --parquet

      - name: Publish delta
        run: |
          DATE="$(date -u +%F)"
          python -m caselaw_pipeline.cli publish-delta --build-dir _build --date "$DATE" --parquet

      - name: Append delta to dataset
        run: |
          DATE="$(date -u +%F)"
          python -m caselaw_pipeline.cli append-to-data --build-dir _build --date "$DATE"

      # ── Consolidation: merge deltas and update snapshot ──────────────
      - name: Consolidate data shards
        run: |
          python -m caselaw_pipeline.cli consolidate-data \
            --build-dir _build
        continue-on-error: true

      - name: Update SQLite snapshot
        run: |
          python -m caselaw_pipeline.cli consolidate-weekly \
            --build-dir _build \
            --zstd-level 10
        continue-on-error: true

      - name: Write job summary
        run: |
          echo "## Daily Update Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### New Decisions Scraped (delta)" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Count |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|------:|" >> $GITHUB_STEP_SUMMARY
          echo "| Decisions Scraped | ${{ steps.stats.outputs.total }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Federal | ${{ steps.stats.outputs.federal }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Cantonal | ${{ steps.stats.outputs.cantonal }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Cantons | ${{ steps.stats.outputs.cantons }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Links" >> $GITHUB_STEP_SUMMARY
          echo "- [HuggingFace Dataset](https://huggingface.co/datasets/voilaj/swiss-caselaw)" >> $GITHUB_STEP_SUMMARY
