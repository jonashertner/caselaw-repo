name: Weekly Dataset Consolidation

on:
  schedule:
    # Run Monday 2 AM UTC â€” after daily (4 AM Sun) and weekly entscheidsuche (5 AM Sun)
    - cron: '0 2 * * 1'
  workflow_dispatch:

env:
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_DATASET_REPO: ${{ secrets.HF_DATASET_REPO }}
  LOG_LEVEL: INFO

# Prevent concurrent runs of data update workflows
concurrency:
  group: data-update
  cancel-in-progress: false

jobs:
  consolidate:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: pipeline/requirements.txt

      - name: Install pipeline deps
        run: |
          python -m pip install --upgrade pip
          pip install -r pipeline/requirements.txt
          pip install -e pipeline

      - name: Consolidate data shards
        run: |
          python -m caselaw_pipeline.cli consolidate-data \
            --build-dir _build

      - name: Write job summary
        run: |
          echo "## Weekly Dataset Consolidation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Downloaded all data/*.parquet shards, deduplicated by ID, re-uploaded clean shards." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- [HuggingFace Dataset](https://huggingface.co/datasets/voilaj/swiss-caselaw)" >> $GITHUB_STEP_SUMMARY
